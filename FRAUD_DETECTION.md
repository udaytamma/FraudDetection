# Fraud Detection Platform - Project Reference

> This document is the single source of truth for the Fraud Detection Platform project. Claude Code should read this file when working on fraud detection implementation.

## Project Overview

**Purpose:** Real-time fraud detection system for payment transactions, designed for interview preparation (Principal TPM/Senior TPM at Mag7 companies) and potential implementation.

**Status:** MVP implemented (FastAPI + Redis + Postgres). Target architecture is documented as a future phase.

**Location:** `/Users/omega/Projects/FraudDetection`

---

## Key Files

| File | Purpose |
|------|---------|
| `PRDs/Master Prompt - Fraud Detection.txt` | **PRIMARY REFERENCE** - The master prompt that defines all design constraints, practitioner requirements, and system architecture |
| `PRDs/Fraud-Detection-PRD - Perplexity.md` | PRD generated by Perplexity |
| `PRDs/Fraud_Detection_PRD_v1.2 - Opus.docx` | PRD generated by Claude Opus |
| `PRDs/Capstone - Fraud Detection - PRD-Gemini.pdf` | PRD generated by Gemini |

---

## Master Prompt Summary

The master prompt (`PRDs/Master Prompt - Fraud Detection.txt`) establishes:

### 10 Practitioner Constraints

1. **Net Revenue Optimization** - Maximize revenue minus fraud losses, not just minimize fraud
2. **Policy > ML** - Business controls decisions without engineering deploys
3. **Fraud Taxonomy** - Criminal, friendly, and merchant error need different responses
4. **Evidence as Revenue** - Disputes are winnable with proper evidence
5. **Velocity > Sophistication** - Attacks cause damage in minutes, not days
6. **Adversarial Resilience** - Fraudsters probe and adapt to thresholds
7. **Economic Optimization** - Thresholds need data-driven tuning
8. **Model Lifecycle** - Production ML needs rollback and monitoring
9. **PCI/PII Governance** - Compliance boundaries shape architecture
10. **Multi-Signal Integration** - Future phases need telco/MSP signals

### Non-Negotiable Numbers

- **<200ms** end-to-end latency for online decisioning
- **Exactly-once** business effects (no duplicate charges or blocks)
- **>92%** approval rate target
- **<10%** false positive rate among blocks

Implementation note: exactly-once decision responses are enforced via Redis + Postgres idempotency records (capstone scope).

---

## Capstone Scope & Intent

This repository intentionally implements a **capstone MVP** that demonstrates end‑to‑end decisioning, evidence capture, and policy management with minimal infrastructure. The Kafka/Flink/Feast/OPA/Seldon architecture described below is a **target reference design** for Phase 2, not a claim of current implementation.

---

## Technology Stack

### Current MVP (Implemented)

| Component | Technology | Why Selected |
|-----------|------------|--------------|
| API | FastAPI | Simple, high-performance request handling |
| State Store | Redis | Low-latency velocity counters + entity profiles |
| Evidence Store | PostgreSQL | Structured evidence + audit trail |
| Policy Engine | YAML + custom evaluator | Hot-reloadable and versioned for demo |
| Monitoring | Prometheus client + in-app `/metrics` | Lightweight metrics for demo |

### Target Architecture (Phase 2, Not Implemented)

| Component | Technology | Why Selected |
|-----------|------------|--------------|
| Event Streaming | Kafka | Exactly-once semantics, partition-by-key ordering, log retention for replay |
| Stream Processing | Flink | True event-by-event processing, native sliding windows, checkpoint recovery |
| Feature Store | Feast + Delta Lake | Open source, Spark integration, point-in-time correctness |
| Policy Engine | OPA + Custom Wrapper | Hot-reloadable Rego policies, version-controlled, audit trail |
| Model Serving | Seldon | Kubernetes-native, canary deployments, explainability |
| Evidence Store | PostgreSQL + S3 | Structured queries for disputes, cheap blob storage for documents |
| Monitoring | Prometheus + Grafana | Standard stack, PromQL flexibility, alerting |

---

## Architecture Overview

```
                    ┌─────────────────────────────────────────────────────────┐
                    │                    KAFKA CLUSTER                         │
                    │  payment-events → fraud-decisions → evidence-requests   │
                    └─────────────────────────────────────────────────────────┘
                              │                    ▲
                              ▼                    │
┌──────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌──────────────┐
│   INGESTION  │───▶│  FLINK CLUSTER  │───▶│  DECISION SVC   │───▶│  EVIDENCE    │
│   GATEWAY    │    │  (Features)      │    │  (Policy+ML)    │    │  VAULT       │
└──────────────┘    └─────────────────┘    └─────────────────┘    └──────────────┘
                              │                    │
                              ▼                    ▼
                    ┌─────────────────┐    ┌─────────────────┐
                    │  REDIS CLUSTER  │    │  MODEL SERVICE  │
                    │  (Real-time)    │    │  (Seldon)       │
                    └─────────────────┘    └─────────────────┘
                              │
                              ▼
                    ┌─────────────────┐
                    │  FEATURE STORE  │
                    │  (Feast+Delta)  │
                    └─────────────────┘
```

---

## RBAC & Security Architecture

### Authentication Model

The API implements a three-tier token authentication system:

| Token Type | Environment Variable | Scope |
|------------|---------------------|-------|
| API Token | `API_TOKEN` | Decision endpoints, policy reads |
| Admin Token | `ADMIN_TOKEN` | Policy mutation, configuration changes |
| Metrics Token | `METRICS_TOKEN` | Observability endpoints |

### Endpoint Protection

| Endpoint | Required Token | Dependency Function |
|----------|---------------|---------------------|
| `POST /decide` | API_TOKEN | `require_api_token` |
| `GET /policy/*` | API_TOKEN | `require_api_token` |
| `POST /policy/reload` | ADMIN_TOKEN | `require_admin_token` |
| `GET /metrics` | METRICS_TOKEN | `require_metrics_token` |
| `GET /metrics/summary` | METRICS_TOKEN | `require_metrics_token` |
| `GET /health` | None | Open |

### Supported Headers

```
Authorization: Bearer <token>
X-API-Key: <token>
```

### Capstone Security Limitations

These are documented as accepted risks for capstone scope:

1. **No timing attack resistance** - Direct string comparison in `src/api/auth.py` is vulnerable to timing attacks. Production should use `secrets.compare_digest()`.

2. **No audit logging** - Authentication attempts (success/failure) are not logged. Production needs auth audit trail for security monitoring.

3. **No token rotation** - Same token until manually changed. Production should support token refresh/rotation.

4. **No per-user identity** - Shared tokens without individual accountability. Production should integrate with identity provider.

---

## Evidence Vault & Compliance (Capstone)

### Two-Table Architecture

| Table | Purpose | Data Stored |
|-------|---------|-------------|
| `transaction_evidence` | Primary evidence | HMAC-hashed identifiers, scores, decisions, timestamps |
| `evidence_vault` | Sensitive data | Fernet-encrypted raw identifiers (device ID, IP, fingerprint) |

### Data Flow

```
                  ┌─────────────────────────┐
                  │     /decide request     │
                  └───────────┬─────────────┘
                              │
                              ▼
              ┌───────────────────────────────┐
              │    Decision + Scoring         │
              └───────────────┬───────────────┘
                              │
              ┌───────────────┴───────────────┐
              │                               │
              ▼                               ▼
    ┌─────────────────┐             ┌─────────────────┐
    │ transaction_    │             │ evidence_vault  │
    │ evidence        │             │ (Encrypted)     │
    │ (HMAC hashed)   │             │                 │
    └─────────────────┘             └─────────────────┘
```

### Encryption Details

- **Vault Encryption**: Fernet symmetric encryption (AES-128-CBC + HMAC-SHA256)
- **Identifier Hashing**: HMAC-SHA256 with separate key
- **Key Storage**: Environment variables (`EVIDENCE_VAULT_KEY`, `EVIDENCE_HASH_KEY`)

### Retention Policy

- Default: 730 days (2 years) for chargeback dispute window
- Configurable via `EVIDENCE_RETENTION_DAYS`
- Purge script: `scripts/purge_evidence_vault.py`

### Production Gap: Key Rotation

The capstone does not implement key rotation. For production:
- Version encryption keys
- Re-encrypt vault on rotation
- Maintain key history for decryption of old records

This is a **PCI-aware** design, not a claim of PCI DSS certification.

---

## Data Model

### Core Entities (Derived from Money Flow)

| Entity | Fraud Vector | Storage | Key Fields |
|--------|--------------|---------|------------|
| **Card** | Stolen, enumerated, tested | Redis Hash | first_seen, chargeback_count, distinct_merchants_30d |
| **Device** | Shared across fraud rings, emulated | Redis Hash | distinct_cards_24h (HLL), is_emulator, is_rooted |
| **IP** | Proxied, VPN, datacenter | Redis Hash | distinct_cards_1h, is_datacenter, geo_country |
| **User** | Fake accounts, ATO, friendly fraud | Redis Hash | account_age_days, chargeback_rate_90d, risk_tier |
| **Service** | Telco/MSP account abuse | Redis Hash | total_transactions, first_seen, last_seen |
| **Merchant** | Collusion, high-risk MCC (legacy alias) | Redis Hash | mcc, chargeback_rate, is_high_risk |

> Note: `merchant_*` columns are maintained as a backward-compatible alias for `service_*` in the capstone MVP.

### Event Types

| Event | State Change | Latency Requirement |
|-------|--------------|---------------------|
| Authorization | Transaction created | <200ms decision |
| Capture | Money moves | Async |
| Refund | Reversal | Async |
| Chargeback | Loss recorded | Batch |
| Dispute Outcome | ML training label | Batch |

### Feature Categories

| Category | Computation | Storage | Example |
|----------|-------------|---------|---------|
| Velocity (real-time) | Sliding window counters | Redis ZSET | card_attempts_10m |
| Aggregates (near real-time) | Incremental updates | Redis Hash | user_chargeback_count |
| Historical (batch) | Daily rollups | Feast (future phase; Redis-only in MVP) | user_chargeback_rate_90d |

---

## Decision Logic

### Rule Hierarchy (Priority Order)

1. **Hard Overrides** - Blocklists/allowlists (immediate)
2. **Velocity Circuit Breakers** - Card testing, enumeration
3. **ML Score Thresholds** - Criminal fraud, friendly fraud
4. **Contextual Rules** - High value + new user, geo mismatch
5. **Default Decision** - ALLOW

### Decision Space

| Decision | Business Meaning | Trade-off |
|----------|------------------|-----------|
| ALLOW | Proceed | Revenue captured, fraud risk accepted |
| FRICTION | Request verification (3DS, OTP) | Some abandonment, reduced fraud |
| REVIEW | Hold for analyst | Delay, human cost, higher accuracy |
| BLOCK | Decline | Zero fraud risk, lost revenue if legitimate |

### Profit-Based Thresholds

```
Expected Loss = P(fraud) × (amount + chargeback_fee + penalty + ops_cost)
Expected Gain = P(legitimate) × (revenue from transaction)

If Expected Loss > Expected Gain × risk_tolerance → BLOCK or FRICTION
Else → ALLOW
```

---

## Failure Modes & Fallbacks

| Component | Failure Mode | Fallback | Status |
|-----------|--------------|----------|--------|
| Redis | Node down | Default features (zeroed counters) | Implemented |
| Redis | Cluster failover | Replica failover, then safe mode | Future (single-node MVP) |
| Scoring | N/A | Rule-based only (no ML service yet) | Implemented |
| Policy Engine | Config error | Returns HTTP 500; retains last valid policy in memory | Implemented |
| Evidence Vault | Write failure | Best-effort, log failure (no retry) | Implemented |
| Idempotency (Redis) | Redis unavailable | PostgreSQL fallback | Implemented |
| Background Tasks | Task failure | Fire-and-forget (no retry) | Implemented |

### Idempotency Implementation

```
                     ┌──────────────┐
                     │   Request    │
                     └──────┬───────┘
                            │
                            ▼
                    ┌───────────────┐
                    │ Check Redis   │
                    │ idempotency   │
                    └───────┬───────┘
                            │
              ┌─────────────┴─────────────┐
              │                           │
         Hit (cached)              Miss (new request)
              │                           │
              ▼                           ▼
    ┌─────────────────┐         ┌─────────────────┐
    │ Return cached   │         │ Process request │
    │ decision        │         │ + store result  │
    └─────────────────┘         └────────┬────────┘
                                         │
                                         ▼
                                ┌─────────────────┐
                                │ Write to Redis  │
                                │ + Postgres      │
                                └─────────────────┘
```

**Capstone limitation:** Redis and Postgres idempotency records can diverge on failover. Production should use distributed transaction or saga pattern.

### Safe Mode Behavior

When `SAFE_MODE_ENABLED=true`:
- All scoring, velocity updates, and feature computation bypassed
- Returns configured `SAFE_MODE_DECISION` (default: ALLOW)
- Evidence captured with zeroed scores for auditability
- Prometheus decision metrics and latency are recorded

**Use cases:**
- False positive spikes causing revenue loss
- Deployment of new scoring logic
- Dependency failures (Redis/Postgres degraded)

### Background Task Limitations

Evidence capture and async operations use fire-and-forget background tasks:
- No retry mechanism on failure
- No dead letter queue
- Task failures are logged but not recovered

Production recommendation: Use Celery or similar task queue with retry and DLQ.

### Attack vs Bug Detection

| Signal | Attack | Bug |
|--------|--------|-----|
| Block rate spike | Concentrated on few entities | Spread across all traffic |
| Geographic pattern | Single region | Global |
| User complaints | None | Support tickets spike |
| Time pattern | Sustained | Correlates with deployment |

---

## Testing Strategy

### Testing Pyramid

1. **Unit Tests** - Decision logic, edge cases (seconds)
2. **Integration Tests** - End-to-end flows, idempotency (minutes)
3. **Chaos Tests** - Failure resilience (minutes)
4. **Replay Tests** - Historical accuracy (hours) **← Most Critical**
5. **Load Tests** - Capacity at 2x traffic (hours)

### Historical Replay Testing

- Replay 30 days of transactions through new pipeline
- Get features **as they were at transaction time** (not current)
- Compare decisions to actual fraud outcomes
- Measure: approval rate delta, fraud detection rate, false positive rate

### Go/No-Go Criteria

| Category | Metric | Target | Max Allowed |
|----------|--------|--------|-------------|
| Performance | E2E P99 latency | 150ms | 200ms |
| Performance | Error rate | 0.1% | 1% |
| Accuracy | Approval rate delta | 0% | -2% |
| Accuracy | Fraud detection delta | 0% | -5% |
| Accuracy | False positive delta | 0% | +1% |

---

## Phase Plan

### Phase 1 (Sprint 1-2): Core Pipeline

- Real-time transaction decisioning (<200ms)
- Velocity features (card, device, IP, user)
- Criminal fraud detection
- Policy engine with configurable thresholds
- Evidence vault
- Basic monitoring

### Phase 2 (Sprint 3-4): Optimization

- Automated representment
- Economic optimization UI
- Champion/challenger A/B testing
- Model retraining pipeline

### Phase 3: Extended Detection

- IRSF detection (telco signals)
- ATO detection (session/login data)
- Subscription fraud

---

## Nebula Documentation

Interview preparation materials in Cyrus:

**Location:** `/Users/omega/Projects/Cyrus/src/app/nebula/fraud-detection-thinking/`

**Each section contains:**
1. Thinking Process - Systematic step-by-step derivation
2. Decision Context - Specific choices and rationale
3. Derivation Path - Visual flow of how to arrive at design
4. Interview Application - 2-minute response template

---

## Key Design Principles

1. **Constraints before solutions** - Enumerate non-negotiables first
2. **Design for the whole, build in phases** - Don't solve tomorrow's problem, but don't block it
3. **ML informs, Policy decides** - Separation is non-negotiable
4. **Everything fails** - Design for when, not if
5. **Profit-based thresholds** - Expected value, not fixed cutoffs
6. **Prepare at architect depth, deliver at principal depth** - Adjust based on interviewer cues

---

## CREST Framework (Technology Selection)

| Letter | Meaning | What You Demonstrate |
|--------|---------|---------------------|
| C | Constraints | Start with requirements, not solutions |
| R | Risks & Trade-offs | Understand nothing is free |
| E | Evaluation | Compared alternatives systematically |
| S | Selection | Made a defensible choice |
| T | Testing/Validation | Know how to verify the choice works |

---

## Security Decisions & Accepted Risks

> **Context:** This is a portfolio/demo deployment on Railway. The security posture is intentionally "good enough for demo" -- not enterprise-grade. This section documents what we implemented, what we skipped, and why.

### What We Implemented (Portfolio-Grade)

| Control | Implementation | Status |
|---------|---------------|--------|
| API token auth | Static `API_TOKEN` via `X-API-Key` header on all `/decide` and `/policy/*` endpoints | Implemented |
| Admin token separation | Separate `ADMIN_TOKEN` for policy mutation endpoints (`PUT /policy/thresholds`, `POST /policy/rules`) | Implemented |
| Private networking | Dashboard -> API communication over Railway private network (`api.railway.internal:8080`), not public internet | Implemented |
| Health endpoint (no auth) | `GET /health` is unauthenticated -- allows Railway health checks and external monitoring | By design |
| CORS | `CORS_ALLOW_ORIGINS="*"` -- permissive for demo, would be locked to dashboard domain in production | Accepted risk |
| HTTPS | TLS termination at Railway's edge proxy (Metal Edge) for all public endpoints | Provided by Railway |

### What Enterprise Would Require (Not Implemented)

| Enterprise Control | Current Gap | Risk Accepted |
|-------------------|------------|---------------|
| **Secret management** | API_TOKEN is copy-pasted between api and dashboard services. Should use Railway Shared Variables (`${{api.API_TOKEN}}`) or a secrets vault (HashiCorp Vault, AWS Secrets Manager) | Token rotation requires updating 2 services manually |
| **mTLS / service mesh** | Inter-service comms use static token over private networking. Enterprise uses mutual TLS (Istio, Linkerd) where both sides verify certificates | A compromised service on the same Railway project could impersonate the dashboard |
| **Token rotation** | No mechanism to rotate tokens without redeploying both services. Enterprise uses short-lived tokens (JWT with expiry) or automatic rotation | Tokens are long-lived; compromise means manual rotation + redeploy |
| **Rate limiting / circuit breaking** | No protection against the dashboard flooding the API. Enterprise uses API gateways (Kong, Envoy) with rate limits per client | A bug in the dashboard could DDoS the API |
| **Audit logging** | No record of which service made which request. Enterprise logs every API call with caller identity, timestamp, and action | Cannot trace who changed policy or when |
| **Secrets in env vars** | Tokens stored as Railway env vars (encrypted at rest by Railway, but visible in dashboard). Enterprise uses dedicated secrets managers with access policies and audit trails | Anyone with Railway project access can read all tokens |
| **Input validation depth** | Basic Pydantic validation on request bodies. Enterprise adds WAF (Web Application Firewall) rules, payload size limits, and schema-level threat detection | Malformed payloads could exploit edge cases in detection logic |

### Why This Is Acceptable for Portfolio

1. **Demonstrates the concept:** Token-based auth with separate API/Admin scopes shows understanding of service-to-service authentication patterns
2. **Private networking:** Using Railway's internal network for dashboard-to-API communication is the correct architecture -- just missing mTLS
3. **Layered auth:** The `require_api_token` / `require_admin_token` / `require_metrics_token` separation shows role-based access control thinking
4. **The gaps are documentable:** In an interview, being able to articulate these gaps and their enterprise solutions demonstrates senior-level security awareness

### Improvement Path (If Moving to Production)

1. **Immediate:** Convert `API_TOKEN` on dashboard to `${{api.API_TOKEN}}` Railway variable reference (single source of truth)
2. **Short-term:** Add rate limiting middleware to FastAPI (`slowapi` or custom), add structured audit logging
3. **Medium-term:** Replace static tokens with JWT (short-lived, auto-rotating), add API gateway
4. **Long-term:** Service mesh (Istio) for mTLS, secrets vault integration, WAF

---

## Quick Reference

**Master Prompt:** `/Users/omega/Projects/FraudDetection/PRDs/Master Prompt - Fraud Detection.txt`

**Nebula Pages:** `/Users/omega/Projects/Cyrus/src/app/nebula/fraud-detection-thinking/`

---

*Last Updated: February 08, 2026*
